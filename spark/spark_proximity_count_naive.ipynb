{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5d528ba5-31af-40fe-b35e-b1dd6839a020;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 231ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5d528ba5-31af-40fe-b35e-b1dd6839a020\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/04 15:48:53 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/05/04 15:49:02 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar added multiple times to distributed cache.\n",
      "23/05/04 15:49:02 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar added multiple times to distributed cache.\n",
      "23/05/04 15:49:02 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                      (15 + 8) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=65>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/context.py\", line 362, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/context.py\", line 1447, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o28.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o133.execute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m\n\u001b[1;32m     63\u001b[0m     delta_table\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mold\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     64\u001b[0m         \u001b[39m.\u001b[39mmerge(\n\u001b[1;32m     65\u001b[0m             df\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mnew\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         ) \\\n\u001b[1;32m     86\u001b[0m         \u001b[39m.\u001b[39mexecute()\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m# # Upsert delta table\u001b[39;00m\n\u001b[1;32m     62\u001b[0m delta_table \u001b[39m=\u001b[39m DeltaTable\u001b[39m.\u001b[39mforPath(spark, TABLE_PATH)\n\u001b[0;32m---> 63\u001b[0m delta_table\u001b[39m.\u001b[39;49malias(\u001b[39m\"\u001b[39;49m\u001b[39mold\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     64\u001b[0m     \u001b[39m.\u001b[39;49mmerge(\n\u001b[1;32m     65\u001b[0m         df\u001b[39m.\u001b[39;49malias(\u001b[39m\"\u001b[39;49m\u001b[39mnew\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     66\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mold.Business_id = new.Business_id\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     67\u001b[0m     ) \\\n\u001b[1;32m     68\u001b[0m     \u001b[39m.\u001b[39;49mwhenMatchedUpdate(\u001b[39mset\u001b[39;49m\u001b[39m=\u001b[39;49m\n\u001b[1;32m     69\u001b[0m         {\n\u001b[1;32m     70\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mBusiness_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Business_id\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     71\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLatitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Latitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     72\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLongitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Longitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     73\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mStars\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Stars\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     74\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.ProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     75\u001b[0m         }\n\u001b[1;32m     76\u001b[0m     ) \\\n\u001b[1;32m     77\u001b[0m     \u001b[39m.\u001b[39;49mwhenNotMatchedInsert(values\u001b[39m=\u001b[39;49m\n\u001b[1;32m     78\u001b[0m         {\n\u001b[1;32m     79\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mBusiness_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Business_id\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     80\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLatitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Latitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     81\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLongitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Longitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     82\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mStars\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Stars\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     83\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.ProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     84\u001b[0m         }\n\u001b[1;32m     85\u001b[0m     ) \\\n\u001b[1;32m     86\u001b[0m     \u001b[39m.\u001b[39;49mexecute()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/delta/tables.py:1022\u001b[0m, in \u001b[0;36mDeltaMergeBuilder.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m0.4\u001b[39m)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m    Execute the merge operation based on the built matched and not matched actions.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \n\u001b[1;32m   1020\u001b[0m \u001b[39m    See :py:class:`~delta.tables.DeltaMergeBuilder` for complete usage details.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jbuilder\u001b[39m.\u001b[39;49mexecute()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o133.execute"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====>                                                   (16 + 8) / 200]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, lit, col, broadcast, count, sum\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import *\n",
    "import pyspark \n",
    "from delta import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# This took 9mins with 10k rows\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Constants\n",
    "    DISTANCE = 2 # km\n",
    "    TABLE_PATH = \"hdfs:///project/data/business_data/delta_table_proximity_count\"\n",
    "    CSV_PATH = \"hdfs:///project/data/business_data/yelp_academic_dataset_business.csv\"\n",
    "\n",
    "    # Calculate the haversine distance between two sets of latitudes and longitudes\n",
    "    @udf(returnType=FloatType())\n",
    "    def haversine_distance(lat1, lon1, lat2, lon2) -> float:\n",
    "        R = 6371  # radius of the earth in kilometers\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        distance = R * c\n",
    "        return distance\n",
    "    \n",
    "    @udf(returnType=IntegerType())\n",
    "    def proximity(distance):\n",
    "        if distance <= DISTANCE: return 1\n",
    "        return 0\n",
    "    \n",
    "    # Initiate spark\n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"spark_proximity_count_naive\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.executor.cores\", 4)\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # Create dataframe from csv file\n",
    "    schema = \"Business_id STRING, Latitude FLOAT, Longitude FLOAT, Stars FLOAT\"\n",
    "    df = spark.read.csv(CSV_PATH, schema=schema).repartition(200, \"Business_id\")\n",
    "\n",
    "    # Combination of all business pairs\n",
    "    df_pairs = df.toDF(\"Business_id1\", \"Latitude1\", \"Longitude1\", \"Stars1\").crossJoin(df.toDF(\"Business_id2\", \"Latitude2\", \"Longitude2\", \"Stars2\"))\n",
    "\n",
    "    # Calculate haversine distances\n",
    "    df_pairs = df_pairs.withColumn(\"Proximity\", proximity(haversine_distance(df_pairs.Latitude1, df_pairs.Longitude1, df_pairs.Latitude2, df_pairs.Longitude2)))\n",
    "    df_proximity = df_pairs.groupBy(col(\"Business_id1\")).agg(sum(\"Proximity\").alias(\"ProximityCount\"))\n",
    "\n",
    "    df = df.join(df_proximity, on=df.Business_id == df_proximity.Business_id1, how=\"inner\").drop(\"Business_id1\")\n",
    "\n",
    "\n",
    "    # If no delta table exists, save and exit\n",
    "    if not DeltaTable.isDeltaTable(spark, TABLE_PATH):\n",
    "        df.write.format(\"delta\").save(TABLE_PATH)\n",
    "        return\n",
    "        \n",
    "    # # Upsert delta table\n",
    "    delta_table = DeltaTable.forPath(spark, TABLE_PATH)\n",
    "    delta_table.alias(\"old\") \\\n",
    "        .merge(\n",
    "            df.alias(\"new\"),\n",
    "            \"old.Business_id = new.Business_id\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set=\n",
    "            {\n",
    "                \"Business_id\": \"new.Business_id\",\n",
    "                \"Latitude\": \"new.Latitude\",\n",
    "                \"Longitude\": \"new.Longitude\",\n",
    "                \"Stars\": \"new.Stars\",\n",
    "                \"ProximityCount\": \"new.ProximityCount\"\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values=\n",
    "            {\n",
    "                \"Business_id\": \"new.Business_id\",\n",
    "                \"Latitude\": \"new.Latitude\",\n",
    "                \"Longitude\": \"new.Longitude\",\n",
    "                \"Stars\": \"new.Stars\",\n",
    "                \"ProximityCount\": \"new.ProximityCount\"\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
