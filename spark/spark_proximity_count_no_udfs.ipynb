{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7401e71d-ce0b-4766-a9ca-bdeb0fa81608;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 1302ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7401e71d-ce0b-4766-a9ca-bdeb0fa81608\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 20:27:37 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/04/26 20:28:03 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar added multiple times to distributed cache.\n",
      "23/04/26 20:28:03 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar added multiple times to distributed cache.\n",
      "23/04/26 20:28:03 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=====================================================> (196 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+-----+--------------+\n",
      "|         Business_id| Latitude|  Longitude|Stars|ProximityCount|\n",
      "+--------------------+---------+-----------+-----+--------------+\n",
      "|--ZVrH2X2QXBFdCil...|  39.9973| -75.292206|  4.5|            30|\n",
      "|--_9CAxgfXZmoFdNI...|27.772844| -82.676155|  3.5|            20|\n",
      "|--hF_3v1JmU9nlu4z...|39.767887|  -86.15835|  4.5|            82|\n",
      "|-02xFuruu85XmDn2x...|32.251038|-110.833176|  4.5|            21|\n",
      "|-0Ym1Wg3bXd_TDz8J...|53.518486| -113.49318|  4.5|            41|\n",
      "|-0fvhILrC9UsQ6gLN...|40.046192|  -75.01509|  4.5|            13|\n",
      "|-1ueCbvIpUPi8KT95...| 39.50442| -119.77959|  4.0|            51|\n",
      "|-2Axhv9AZ_n7qjQef...|36.216606|  -86.59637|  3.5|             2|\n",
      "|-2bLM6nIETD4Mk-rh...|39.768684|  -86.15352|  5.0|            78|\n",
      "|-2ke_JDOpgTZWqirM...| 39.71531|  -86.08123|  3.5|             4|\n",
      "|-3-6BB10tIWNKGEF0...| 32.28854| -110.96314|  4.0|            30|\n",
      "|-3AooxIkg38UyUdlz...|34.421932| -119.70219|  3.0|           172|\n",
      "|-3YTjk_J0pmtcnaqr...|29.997177|  -90.18693|  2.5|            29|\n",
      "|-49YlJ2GGzgmQ71FX...| 39.81276| -74.929695|  4.0|            11|\n",
      "|-4VQum5gCgEfSZcFy...|30.018341| -90.239784|  4.0|            25|\n",
      "|-4dYswJy7SPcbcERv...|39.940403|   -75.1933|  2.5|            85|\n",
      "|-4qjiD8Rk5yNDHM6H...|53.546898| -113.41098|  5.0|             2|\n",
      "|-5HAZGNFs5-MkwT1i...|28.122936|  -82.46324|  5.0|             4|\n",
      "|-5iuX3tPbwH5LpoWN...|28.064669| -82.559235|  4.5|            15|\n",
      "|-5jrwZnndGs9q3akd...|53.523327|-113.624794|  3.5|            28|\n",
      "+--------------------+---------+-----------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, acos, cos, radians, sin\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import *\n",
    "import pyspark \n",
    "from delta import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Constants\n",
    "    DISTANCE = 2 # km\n",
    "    TABLE_PATH = \"hdfs:///project/data/business_data/delta_table_proximity_count\"\n",
    "    CSV_PATH = \"hdfs:///project/data/business_data/yelp_academic_dataset_business.csv\"\n",
    "    \n",
    "    # Initiate spark\n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"spark_proximity_count_no_udfs\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.executor.cores\", 4)\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # Create dataframe from csv file\n",
    "    schema = \"Business_id STRING, Latitude FLOAT, Longitude FLOAT, Stars FLOAT\"\n",
    "    df = spark.read.csv(CSV_PATH, schema=schema).repartition(200, \"Business_id\")\n",
    "\n",
    "    # Combination of all business pairs\n",
    "    # df_pairs = df.toDF(\"Business_id1\", \"Latitude1\", \"Longitude1\", \"Stars1\").crossJoin(df.toDF(\"Business_id2\", \"Latitude2\", \"Longitude2\", \"Stars2\"))\n",
    "\n",
    "    df.createOrReplaceTempView(\"df1\")\n",
    "    df.createOrReplaceTempView(\"df2\")\n",
    "    df_pairs = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "        df1.Business_id AS Business_id1\n",
    "        ,df2.Business_id AS Business_id2\n",
    "        ,df1.Latitude AS Latitude1\n",
    "        ,df1.Longitude AS Longitude1\n",
    "        ,df2.Latitude AS Latitude2\n",
    "        ,df2.Longitude AS Longitude2\n",
    "        FROM df1, df2\n",
    "        WHERE df1.Business_id <> df2.Business_id\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Calculate haversine distances\n",
    "    df_pairs = df_pairs.withColumn(\"WithinProximity\", \n",
    "        ((acos(sin(radians(\"Latitude1\")) * sin(radians(\"Latitude2\")) +\n",
    "            cos(radians(\"Latitude1\")) * cos(radians(\"Latitude2\")) *\n",
    "            cos(radians(\"Longitude2\") - radians(\"Longitude1\"))) * 6371\n",
    "    ) <= DISTANCE).cast(\"integer\"))\n",
    "    df_proximity = df_pairs.groupBy(col(\"Business_id1\")).agg(sum(\"WithinProximity\").alias(\"ProximityCount\"))\n",
    "    df = df.join(df_proximity, on=df.Business_id == df_proximity.Business_id1, how=\"inner\").drop(\"Business_id1\")\n",
    "\n",
    "    # If no delta table exists, save and exit\n",
    "    if not DeltaTable.isDeltaTable(spark, TABLE_PATH):\n",
    "        df.write.format(\"delta\").save(TABLE_PATH)\n",
    "        return\n",
    "        \n",
    "    # # Upsert delta table\n",
    "    delta_table = DeltaTable.forPath(spark, TABLE_PATH)\n",
    "    delta_table.alias(\"old\") \\\n",
    "        .merge(\n",
    "            df.alias(\"new\"),\n",
    "            \"old.Business_id = new.Business_id\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set=\n",
    "            {\n",
    "                \"Business_id\": \"new.Business_id\",\n",
    "                \"Latitude\": \"new.Latitude\",\n",
    "                \"Longitude\": \"new.Longitude\",\n",
    "                \"Stars\": \"new.Stars\",\n",
    "                \"ProximityCount\": \"new.ProximityCount\"\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values=\n",
    "            {\n",
    "                \"Business_id\": \"new.Business_id\",\n",
    "                \"Latitude\": \"new.Latitude\",\n",
    "                \"Longitude\": \"new.Longitude\",\n",
    "                \"Stars\": \"new.Stars\",\n",
    "                \"ProximityCount\": \"new.ProximityCount\"\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
