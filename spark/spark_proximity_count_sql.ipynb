{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2aaf48a7-e306-42d4-b673-7013bb03f718;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 232ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2aaf48a7-e306-42d4-b673-7013bb03f718\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 20:46:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/04/26 20:46:50 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar added multiple times to distributed cache.\n",
      "23/04/26 20:46:50 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar added multiple times to distributed cache.\n",
      "23/04/26 20:46:50 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.age 5:>              (0 + 0) / 40000]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=65>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/context.py\", line 362, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/context.py\", line 1447, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o28.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "[Stage 4:>             (60 + 8) / 40000][Stage 5:>              (0 + 0) / 40000]\r"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o105.execute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m\n\u001b[1;32m     90\u001b[0m     delta_table\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mold\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     91\u001b[0m         \u001b[39m.\u001b[39mmerge(\n\u001b[1;32m     92\u001b[0m             df\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mnew\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ) \\\n\u001b[1;32m    113\u001b[0m         \u001b[39m.\u001b[39mexecute()\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m# # Upsert delta table\u001b[39;00m\n\u001b[1;32m     89\u001b[0m delta_table \u001b[39m=\u001b[39m DeltaTable\u001b[39m.\u001b[39mforPath(spark, TABLE_PATH)\n\u001b[0;32m---> 90\u001b[0m delta_table\u001b[39m.\u001b[39;49malias(\u001b[39m\"\u001b[39;49m\u001b[39mold\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     91\u001b[0m     \u001b[39m.\u001b[39;49mmerge(\n\u001b[1;32m     92\u001b[0m         df\u001b[39m.\u001b[39;49malias(\u001b[39m\"\u001b[39;49m\u001b[39mnew\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     93\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mold.Business_id = new.Business_id\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     94\u001b[0m     ) \\\n\u001b[1;32m     95\u001b[0m     \u001b[39m.\u001b[39;49mwhenMatchedUpdate(\u001b[39mset\u001b[39;49m\u001b[39m=\u001b[39;49m\n\u001b[1;32m     96\u001b[0m         {\n\u001b[1;32m     97\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mBusiness_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Business_id\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     98\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLatitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Latitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     99\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLongitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Longitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    100\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mStars\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Stars\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    101\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.ProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    102\u001b[0m         }\n\u001b[1;32m    103\u001b[0m     ) \\\n\u001b[1;32m    104\u001b[0m     \u001b[39m.\u001b[39;49mwhenNotMatchedInsert(values\u001b[39m=\u001b[39;49m\n\u001b[1;32m    105\u001b[0m         {\n\u001b[1;32m    106\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mBusiness_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Business_id\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    107\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLatitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Latitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    108\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mLongitude\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Longitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    109\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mStars\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.Stars\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    110\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mnew.ProximityCount\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    111\u001b[0m         }\n\u001b[1;32m    112\u001b[0m     ) \\\n\u001b[1;32m    113\u001b[0m     \u001b[39m.\u001b[39;49mexecute()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/delta/tables.py:1022\u001b[0m, in \u001b[0;36mDeltaMergeBuilder.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m0.4\u001b[39m)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m    Execute the merge operation based on the built matched and not matched actions.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \n\u001b[1;32m   1020\u001b[0m \u001b[39m    See :py:class:`~delta.tables.DeltaMergeBuilder` for complete usage details.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jbuilder\u001b[39m.\u001b[39;49mexecute()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o105.execute"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>             (64 + 8) / 40000][Stage 5:>              (0 + 0) / 40000]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, acos, cos, radians, sin\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import *\n",
    "import pyspark \n",
    "from delta import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Constants\n",
    "    DISTANCE = 2 # km\n",
    "    TABLE_PATH = \"hdfs:///project/data/business_data/delta_table_proximity_count\"\n",
    "    CSV_PATH = \"hdfs:///project/data/business_data/yelp_academic_dataset_business.csv\"\n",
    "    # CSV_PATH = \"hdfs:///project/data/business_data/yelp_academic_dataset_business.csv\"\n",
    "    \n",
    "    # Initiate spark\n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"spark_proximity_count_sql\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.executor.cores\", 4)\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # Create dataframe from csv file\n",
    "    schema = \"Business_id STRING, Latitude FLOAT, Longitude FLOAT, Stars FLOAT\"\n",
    "    df = spark.read.csv(CSV_PATH, schema=schema).repartition(200, \"Business_id\")\n",
    "\n",
    "    # Combination of all business pairs\n",
    "    df.createOrReplaceTempView(\"df1\")\n",
    "    df.createOrReplaceTempView(\"df2\")\n",
    "    df_pairs = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "        df1.Business_id AS Business_id1\n",
    "        ,df2.Business_id AS Business_id2\n",
    "        ,df1.Latitude AS Latitude1\n",
    "        ,df1.Longitude AS Longitude1\n",
    "        ,df2.Latitude AS Latitude2\n",
    "        ,df2.Longitude AS Longitude2\n",
    "        FROM df1, df2\n",
    "        WHERE df1.Business_id <> df2.Business_id\n",
    "        AND df1.Business_id < df2.Business_id\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Calculate haversine distances\n",
    "    df_pairs = df_pairs.withColumn(\"WithinProximity\", \n",
    "        ((acos(sin(radians(\"Latitude1\")) * sin(radians(\"Latitude2\")) +\n",
    "            cos(radians(\"Latitude1\")) * cos(radians(\"Latitude2\")) *\n",
    "            cos(radians(\"Longitude2\") - radians(\"Longitude1\"))) * 6371\n",
    "    ) <= DISTANCE).cast(\"integer\"))\n",
    "\n",
    "    df_pairs.createOrReplaceTempView(\"df\")\n",
    "    df_proximity = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT Business_id, SUM(ProximityCount) AS ProximityCount\n",
    "        FROM\n",
    "        (\n",
    "            SELECT df.Business_id1 AS Business_id, SUM(df.WithinProximity) AS ProximityCount\n",
    "            FROM df\n",
    "            GROUP BY df.Business_id1\n",
    "            UNION\n",
    "            SELECT df.Business_id2 AS Business_id, SUM(df.WithinProximity) AS ProximityCount\n",
    "            FROM df\n",
    "            GROUP BY df.Business_id2\n",
    "        )\n",
    "        GROUP BY Business_id\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    df_proximity.createOrReplaceTempView(\"df_proximity\")\n",
    "    df = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT df.*, df_proximity.ProximityCount\n",
    "        FROM df\n",
    "        INNER JOIN df_proximity\n",
    "        ON df.Business_id = df_proximity.Business_id\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # If no delta table exists, save and exit\n",
    "    if not DeltaTable.isDeltaTable(spark, TABLE_PATH):\n",
    "        df.write.format(\"delta\").save(TABLE_PATH)\n",
    "        return\n",
    "        \n",
    "    # # Upsert delta table\n",
    "    delta_table = DeltaTable.forPath(spark, TABLE_PATH)\n",
    "    delta_table.alias(\"old\") \\\n",
    "        .merge(\n",
    "            df.alias(\"new\"),\n",
    "            \"old.Business_id = new.Business_id\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set=\n",
    "            {\n",
    "                \"Business_id\": \"new.Business_id\",\n",
    "                \"Latitude\": \"new.Latitude\",\n",
    "                \"Longitude\": \"new.Longitude\",\n",
    "                \"Stars\": \"new.Stars\",\n",
    "                \"ProximityCount\": \"new.ProximityCount\"\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values=\n",
    "            {\n",
    "                \"Business_id\": \"new.Business_id\",\n",
    "                \"Latitude\": \"new.Latitude\",\n",
    "                \"Longitude\": \"new.Longitude\",\n",
    "                \"Stars\": \"new.Stars\",\n",
    "                \"ProximityCount\": \"new.ProximityCount\"\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
